{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Hackathon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NKalavros/Copehnagen-biohackathon/blob/main/Copy_of_Hackathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgxNIW6ctkh-",
        "outputId": "ddbf8114-c187-4ec1-9ebf-44cdf2313db2"
      },
      "source": [
        "#Linux stuff\n",
        "!pip install seqvec\n",
        "\n",
        "!wget https://rostlab.org/~deepppi/seqvec.zip\n",
        "!unzip seqvec.zip\n",
        "\n",
        "!wget https://rostlab.org/~deepppi/seqvec_checkpoint.tar.gz\n",
        "!tar -zxf seqvec_checkpoint.tar.gz\n",
        "\n",
        "!rm seqvec.zip seqvec_checkpoint.tar.gz\n",
        "\n",
        "!wget https://www.dropbox.com/s/snl3v972umpy0k4/hackathon.csv?dl=0 -O hackathon.csv\n",
        "\n",
        "!!pip install 'umap-learn==0.3.10'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqvec in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: gevent==1.4.0 in /usr/local/lib/python3.7/dist-packages (from seqvec) (1.4.0)\n",
            "Requirement already satisfied: h5py<3.0.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from seqvec) (2.10.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from seqvec) (4.41.1)\n",
            "Requirement already satisfied: allennlp<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from seqvec) (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from seqvec) (1.19.5)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from seqvec) (1.8.1+cu101)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.7/dist-packages (from gevent==1.4.0->seqvec) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py<3.0.0,>=2.10.0->seqvec) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (1.4.1)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (2.0.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (0.5.3)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (1.1.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (2018.9)\n",
            "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (0.22.2.post1)\n",
            "Requirement already satisfied: flaky in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (3.7.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (2.23.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (1.2.0)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (2.1.9)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (0.4.1)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (0.17.0)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (0.6.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (3.6.4)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (2.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (1.17.57)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (3.2.2)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (1.1)\n",
            "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (0.8.1)\n",
            "Requirement already satisfied: conllu==1.3.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (1.3.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (3.2.5)\n",
            "Requirement already satisfied: pytorch-transformers==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (1.1.0)\n",
            "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (0.13.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (6.0.1)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (4.1.1)\n",
            "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from allennlp<0.10.0,>=0.9.0->seqvec) (3.0.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->seqvec) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp<0.10.0,>=0.9.0->seqvec) (3.10.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp<0.10.0,>=0.9.0->seqvec) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp<0.10.0,>=0.9.0->seqvec) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp<0.10.0,>=0.9.0->seqvec) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp<0.10.0,>=0.9.0->seqvec) (1.1.0)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=0.8.0->allennlp<0.10.0,>=0.9.0->seqvec) (1.8.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp<0.10.0,>=0.9.0->seqvec) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<0.10.0,>=0.9.0->seqvec) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<0.10.0,>=0.9.0->seqvec) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<0.10.0,>=0.9.0->seqvec) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp<0.10.0,>=0.9.0->seqvec) (2.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp<0.10.0,>=0.9.0->seqvec) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp<0.10.0,>=0.9.0->seqvec) (0.8.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp<0.10.0,>=0.9.0->seqvec) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp<0.10.0,>=0.9.0->seqvec) (0.9.6)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp<0.10.0,>=0.9.0->seqvec) (7.0.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp<0.10.0,>=0.9.0->seqvec) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp<0.10.0,>=0.9.0->seqvec) (1.0.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp<0.10.0,>=0.9.0->seqvec) (2019.12.20)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<0.10.0,>=0.9.0->seqvec) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<0.10.0,>=0.9.0->seqvec) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<0.10.0,>=0.9.0->seqvec) (20.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<0.10.0,>=0.9.0->seqvec) (8.7.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<0.10.0,>=0.9.0->seqvec) (1.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<0.10.0,>=0.9.0->seqvec) (56.0.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp<0.10.0,>=0.9.0->seqvec) (3.12.4)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.57 in /usr/local/lib/python3.7/dist-packages (from boto3->allennlp<0.10.0,>=0.9.0->seqvec) (1.20.57)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->allennlp<0.10.0,>=0.9.0->seqvec) (0.4.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->allennlp<0.10.0,>=0.9.0->seqvec) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp<0.10.0,>=0.9.0->seqvec) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp<0.10.0,>=0.9.0->seqvec) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp<0.10.0,>=0.9.0->seqvec) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp<0.10.0,>=0.9.0->seqvec) (2.4.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.1.0->allennlp<0.10.0,>=0.9.0->seqvec) (0.1.95)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp<0.10.0,>=0.9.0->seqvec) (0.2.5)\n",
            "Requirement already satisfied: typing-utils>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from overrides->allennlp<0.10.0,>=0.9.0->seqvec) (0.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp<0.10.0,>=0.9.0->seqvec) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp<0.10.0,>=0.9.0->seqvec) (1.1.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp<0.10.0,>=0.9.0->seqvec) (1.2.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp<0.10.0,>=0.9.0->seqvec) (2.6.1)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp<0.10.0,>=0.9.0->seqvec) (1.2.4)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp<0.10.0,>=0.9.0->seqvec) (2.9.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp<0.10.0,>=0.9.0->seqvec) (0.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp<0.10.0,>=0.9.0->seqvec) (20.9)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp<0.10.0,>=0.9.0->seqvec) (2.1.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp<0.10.0,>=0.9.0->seqvec) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp<0.10.0,>=0.9.0->seqvec) (1.1.4)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.26.4\n",
            "    Uninstalling urllib3-1.26.4:\n",
            "      Successfully uninstalled urllib3-1.26.4\n",
            "Successfully installed urllib3-1.25.11\n",
            "--2021-04-25 08:38:52--  https://rostlab.org/~deepppi/seqvec.zip\n",
            "Resolving rostlab.org (rostlab.org)... 131.159.28.73\n",
            "Connecting to rostlab.org (rostlab.org)|131.159.28.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 347361261 (331M) [application/zip]\n",
            "Saving to: ‘seqvec.zip’\n",
            "\n",
            "seqvec.zip          100%[===================>] 331.27M  10.6MB/s    in 33s     \n",
            "\n",
            "2021-04-25 08:39:26 (10.1 MB/s) - ‘seqvec.zip’ saved [347361261/347361261]\n",
            "\n",
            "Archive:  seqvec.zip\n",
            "replace uniref50_v2/weights.hdf5? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "--2021-04-25 08:43:30--  https://rostlab.org/~deepppi/seqvec_checkpoint.tar.gz\n",
            "Resolving rostlab.org (rostlab.org)... 131.159.28.73\n",
            "Connecting to rostlab.org (rostlab.org)|131.159.28.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 614564923 (586M) [application/x-gzip]\n",
            "Saving to: ‘seqvec_checkpoint.tar.gz’\n",
            "\n",
            "seqvec_checkpoint.t  23%[===>                ] 138.59M  10.7MB/s    eta 47s    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi0BiaOBteza"
      },
      "source": [
        "#Imports\n",
        "import torch\n",
        "import csv\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import adjusted_mutual_info_score as ads\n",
        "from sklearn.metrics import normalized_mutual_info_score as nis\n",
        "from sklearn.metrics import mutual_info_score as mis\n",
        "\n",
        "#Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "#Utils\n",
        "'''Some helper functions for PyTorch, including:\n",
        "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
        "    - msr_init: net parameter initialization.\n",
        "    - progress_bar: progress bar mimic xlua.progress.\n",
        "'''\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "from torch import FloatTensor, LongTensor\n",
        "#Evaluation plots\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import umap\n",
        "\n",
        "#Ops\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtJYyX_QGAAH"
      },
      "source": [
        "#Replacing this class with our own dataset class\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, data, labels, transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.labels[index]\n",
        "        # img = Image.fromarray(img)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "class CDRH3MotifDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    pyTorch Dataloader conform class,\n",
        "    requires implementation of __len__ and __getitem__\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv_path: str, device: str = 'cuda:0'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_path (string): Path to the csv file with CDRH3 motifs.\n",
        "            device (string): device where data is kept in memory\n",
        "        \"\"\"\n",
        "        with open(csv_path, 'r') as file:\n",
        "            reader = csv.reader(file, delimiter='\\t')\n",
        "            next(reader)\n",
        "            classes = set()\n",
        "            data = list()\n",
        "            for line in reader:\n",
        "                data.append((line[0], line[1:]))\n",
        "                classes |= set(line[0])\n",
        "\n",
        "        encoding = {letter: np.eye(len(classes), dtype='float32')[i]\n",
        "                    for i, letter in enumerate(classes)}\n",
        "        label_encoding = {letter: i for i, letter in enumerate(classes)}\n",
        "        dataset = np.zeros((len(data), len(data[0][0]), len(classes)),\n",
        "                           dtype='int')\n",
        "        labels = np.zeros((len(data), len(data[0][0])), dtype='int64')\n",
        "        eval_data = dict()\n",
        "        for i, line in enumerate(data):\n",
        "            for jdx, letter in enumerate(line[0]):\n",
        "                dataset[i, jdx, :] = encoding[letter]\n",
        "                labels[i, jdx] = label_encoding[letter]\n",
        "                eval_data[i] = line[1]\n",
        "\n",
        "        # transform numpy array to torch tensor\n",
        "        # and copy it on the correct device\n",
        "        self.dataset = torch.from_numpy(dataset).to(device)\n",
        "        self.labels = torch.from_numpy(labels).to(device)\n",
        "        self.eval_data = eval_data\n",
        "        self.encoding = encoding\n",
        "        self.label_encoding = label_encoding\n",
        "        self.reverse_encoding = {repr(i): j for j, i in encoding.items()}\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataset)\n",
        "\n",
        "    \"\"\"\n",
        "    __getitem__\n",
        "    Output:\n",
        "        (OneHotEncoding of data, IntegerEncoding of data,\n",
        "        list with epitope, structure and antigen)\n",
        "    \"\"\"\n",
        "    def __getitem__(self, idx: int) -> (FloatTensor):\n",
        "        return (self.dataset[idx, :, :],self.labels[idx])\n",
        "    \n",
        "def masking_noise(data, frac):\n",
        "    \"\"\"\n",
        "    data: Tensor\n",
        "    frac: fraction of unit to be masked out\n",
        "    \"\"\"\n",
        "    data_noise = data.clone()\n",
        "    rand = torch.rand(data.size())\n",
        "    data_noise[rand<frac] = 0\n",
        "    return data_noise\n",
        "\n",
        "\n",
        "class MSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(self.__class__, self).__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return torch.mean(torch.sum((input-target)**2, 1))\n",
        "\n",
        "class BCELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(self.__class__, self).__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return -torch.mean(torch.sum(target*torch.log(torch.clamp(input, min=1e-10))+\n",
        "            (1-target)*torch.log(torch.clamp(1-input, min=1e-10)), 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTHwyiwXGAYO"
      },
      "source": [
        "#Simple DAE\n",
        "class DenoisingAutoencoder(nn.Module):\n",
        "    def __init__(self, in_features, out_features, activation=\"relu\", \n",
        "        dropout=0.2, tied=False):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        if tied:\n",
        "            self.deweight = self.weight.t()\n",
        "        else:\n",
        "            self.deweight = Parameter(torch.Tensor(in_features, out_features))\n",
        "        self.bias = Parameter(torch.Tensor(out_features))\n",
        "        self.vbias = Parameter(torch.Tensor(in_features))\n",
        "        \n",
        "        if activation==\"relu\":\n",
        "            self.enc_act_func = nn.ReLU()\n",
        "        elif activation==\"sigmoid\":\n",
        "            self.enc_act_func = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.bias.data.uniform_(-stdv, stdv)\n",
        "        stdv = 1. / math.sqrt(self.deweight.size(1))\n",
        "        self.deweight.data.uniform_(-stdv, stdv)\n",
        "        self.vbias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.enc_act_func(F.linear(x, self.weight, self.bias)))\n",
        "\n",
        "    def encode(self, x, train=True):\n",
        "        if train:\n",
        "            self.dropout.train()\n",
        "        else:\n",
        "            self.dropout.eval()\n",
        "        return self.dropout(self.enc_act_func(F.linear(x, self.weight, self.bias)))\n",
        "\n",
        "    def encodeBatch(self, dataloader):\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        encoded = []\n",
        "        for batch_idx, (inputs) in enumerate(dataloader):\n",
        "            inputs = inputs.view(inputs.size(0), -1).float()\n",
        "            if use_cuda:\n",
        "                inputs = inputs.cuda()\n",
        "            inputs = Variable(inputs)\n",
        "            hidden = self.encode(inputs, train=False)\n",
        "            encoded.append(hidden.data.cpu())\n",
        "\n",
        "        encoded = torch.cat(encoded, dim=0)\n",
        "        return encoded\n",
        "\n",
        "    def decode(self, x, binary=False):\n",
        "        if not binary:\n",
        "            return F.linear(x, self.deweight, self.vbias)\n",
        "        else:\n",
        "            return F.sigmoid(F.linear(x, self.deweight, self.vbias))\n",
        "\n",
        "    def fit(self, trainloader, lr=0.001, batch_size=235000, num_epochs=10, corrupt=0.3,\n",
        "        loss_type=\"mse\"):\n",
        "        \"\"\"\n",
        "        data_x: FloatTensor\n",
        "        valid_x: FloatTensor\n",
        "        \"\"\"\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        if use_cuda:\n",
        "            self.cuda()\n",
        "        print(\"=====Denoising Autoencoding layer=======\")\n",
        "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
        "        if loss_type==\"mse\":\n",
        "            criterion = MSELoss()\n",
        "        elif loss_type==\"cross-entropy\":\n",
        "            criterion = BCELoss()\n",
        "\n",
        "        # validate\n",
        "        #total_loss = 0.0\n",
        "        #total_num = 0\n",
        "        #for batch_idx, (inputs, _) in enumerate(validloader):\n",
        "        #    inputs = inputs.view(inputs.size(0), -1).float()\n",
        "        #    if use_cuda:\n",
        "        #        inputs = inputs.cuda()\n",
        "        #    inputs = Variable(inputs)\n",
        "        #    hidden = self.encode(inputs)\n",
        "        #    if loss_type==\"cross-entropy\":\n",
        "        #        outputs = self.decode(hidden, binary=True)\n",
        "        #    else:\n",
        "        #        outputs = self.decode(hidden)\n",
        "\n",
        "        #    valid_recon_loss = criterion(outputs, inputs)\n",
        "        #    total_loss += valid_recon_loss.data[0] * len(inputs)\n",
        "        #    total_num += inputs.size()[0]\n",
        "\n",
        "        #valid_loss = total_loss / total_num\n",
        "        #print(\"#Epoch 0: Valid Reconstruct Loss: %.3f\" % (valid_loss))\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # train 1 epoch\n",
        "            train_loss = 0.0\n",
        "            for batch_idx, (inputs) in enumerate(trainloader):\n",
        "                if self.activation != \"tanh\":\n",
        "                    inputs = inputs.view(inputs.size(0), -1).float()\n",
        "                inputs_corr = masking_noise(inputs, corrupt)\n",
        "                if use_cuda:\n",
        "                    inputs = inputs.cuda()\n",
        "                    inputs_corr = inputs_corr.cuda()\n",
        "                optimizer.zero_grad()\n",
        "                inputs = Variable(inputs)\n",
        "                inputs_corr = Variable(inputs_corr)\n",
        "\n",
        "                hidden = self.encode(inputs_corr)\n",
        "                if loss_type==\"cross-entropy\":\n",
        "                    outputs = self.decode(hidden, binary=True)\n",
        "                else:\n",
        "                    outputs = self.decode(hidden)\n",
        "                if self.activation != \"tanh\":\n",
        "                    recon_loss = criterion(outputs, inputs)\n",
        "                else:\n",
        "                    recon_loss = criterion(outputs.reshape(inputs.shape),inputs)\n",
        "                train_loss += recon_loss.data.item()*len(inputs)\n",
        "                recon_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # validate\n",
        "            #valid_loss = 0.0\n",
        "            #for batch_idx, (inputs, _) in enumerate(validloader):\n",
        "            #    inputs = inputs.view(inputs.size(0), -1).float()\n",
        "            #    if use_cuda:\n",
        "            #        inputs = inputs.cuda()\n",
        "            #    inputs = Variable(inputs)\n",
        "            #    hidden = self.encode(inputs, train=False)\n",
        "            #    if loss_type==\"cross-entropy\":\n",
        "            #        outputs = self.decode(hidden, binary=True)\n",
        "            #    else:\n",
        "            #        outputs = self.decode(hidden)\n",
        "            #\n",
        "            #    valid_recon_loss = criterion(outputs, inputs)\n",
        "            #    valid_loss += valid_recon_loss.data[0] * len(inputs)\n",
        "\n",
        "            print(\"#Epoch %3d: Reconstruct Loss: %.3f\" % (\n",
        "                epoch+1, train_loss / len(trainloader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLDtoFMDGAhp"
      },
      "source": [
        "#Moving onto stacked DAE\n",
        "\n",
        "def buildNetwork(layers, activation=\"relu\", dropout=0,embedding = False):\n",
        "    net = []\n",
        "    print(layers)\n",
        "    for i in range(1, len(layers)):\n",
        "        if activation==\"relu\":\n",
        "            net.append(nn.Linear(layers[i-1], layers[i]))\n",
        "            net.append(nn.ReLU())\n",
        "        elif activation==\"tanh\":\n",
        "            new_input_dim = tuple([BATCH_SIZE] + layers[0])\n",
        "            net.append(nn.GRU(input_size = layers[0][1],hidden_size = 32,\n",
        "                              num_layers = layers[1],batch_first = True,dropout = dropout))\n",
        "            break\n",
        "        elif activation==\"sigmoid\":\n",
        "            net.append(nn.Sigmoid())\n",
        "        if activation != \"tanh\":\n",
        "            if dropout > 0:\n",
        "                net.append(nn.Dropout(dropout))\n",
        "    return nn.Sequential(*net)\n",
        "\n",
        "class StackedDAE(nn.Module):\n",
        "    def __init__(self, input_dim=784, z_dim=10, binary=True,\n",
        "        encodeLayer=[400], decodeLayer=[400], activation=\"relu\", \n",
        "        dropout=0, tied=False,embedding = False):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.layers = [input_dim] + encodeLayer + [z_dim]\n",
        "        self.activation = activation\n",
        "        self.dropout = dropout\n",
        "        self.embedding = embedding\n",
        "        self.encoder = buildNetwork([input_dim] + encodeLayer, activation=activation, dropout=dropout,\n",
        "                                    embedding = self.embedding)\n",
        "        if activation==\"tanh\":\n",
        "            self.decoder = buildNetwork([[z_dim,z_dim]] + decodeLayer, activation=activation, dropout=dropout,\n",
        "                                        embedding = self.embedding)\n",
        "        else:\n",
        "            self.decoder = buildNetwork([z_dim] + [decodeLayer], activation=activation, dropout=dropout,\n",
        "                                        embedding = self.embedding)\n",
        "        if self.activation != \"tanh\":\n",
        "            self._enc_mu = nn.Linear(11*20*64, z_dim)\n",
        "        \n",
        "            self._dec = nn.Linear(decodeLayer[-1], input_dim)\n",
        "            self._dec_act = None\n",
        "        else:\n",
        "            self._enc_mu = nn.Linear(32*11,z_dim)\n",
        "            print(input_dim[0], input_dim[1],input_dim[0]*input_dim[1])\n",
        "            self._dec = nn.Linear(32,input_dim[0]*input_dim[1])\n",
        "            self._dec_act = \"None\"\n",
        "        if binary:\n",
        "            self._dec_act = nn.Sigmoid()\n",
        "\n",
        "    def decode(self, z):\n",
        "        if self.activation != \"tanh\":\n",
        "            h = self.decoder(z)\n",
        "            x = self._dec(h)\n",
        "        else:\n",
        "            z_new = z.unsqueeze(1)\n",
        "            #print(\"decoder z shape\",z_new.shape)\n",
        "            h, h_n = self.decoder(z_new)\n",
        "            h_new = h.reshape((h.shape[0],-1))\n",
        "            #print(\"hnew shape\",h_new.shape)\n",
        "            x = self._dec(h_new)\n",
        "            #print(\"x shape\",x.shape)\n",
        "        if self._dec_act is not None:\n",
        "            x = self._dec_act(x)\n",
        "        return x\n",
        "\n",
        "    def loss_function(self, recon_x, x):\n",
        "        loss = -torch.mean(torch.sum(x*torch.log(torch.clamp(recon_x, min=1e-10))+\n",
        "            (1-x)*torch.log(torch.clamp(1-recon_x, min=1e-10)), 1))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        if(self.activation != \"tanh\"):\n",
        "            h = self.encoder(x)\n",
        "            z = self._enc_mu(h)\n",
        "        else:\n",
        "            h,h_n = self.encoder(x)\n",
        "            h_new = h.reshape((h.shape[0],-1))\n",
        "            #print(\"hnew shape\",h_new.shape)\n",
        "            z = self._enc_mu(h_new)\n",
        "            #print(\"z shape\",z.shape)\n",
        "        return z, self.decode(z)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
        "        model_dict = self.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict) \n",
        "        self.load_state_dict(model_dict)\n",
        "\n",
        "    def pretrain(self, trainloader, validloader, lr=0.001, batch_size=128, num_epochs=10, corrupt=0.3, loss_type=\"cross-entropy\"):\n",
        "        trloader = trainloader\n",
        "        valoader = validloader\n",
        "        daeLayers = []\n",
        "        for l in range(1, len(self.layers)):\n",
        "            infeatures = self.layers[l-1]\n",
        "            outfeatures = self.layers[l]\n",
        "            dae = DenoisingAutoencoder(infeatures, outfeatures, activation=self.activation, dropout=self.dropout)\n",
        "            if l==1:\n",
        "                dae.fit(trloader, valoader, lr=lr, batch_size=batch_size, num_epochs=num_epochs, corrupt=corrupt, loss_type=loss_type)\n",
        "            else:\n",
        "                if self.activation==\"sigmoid\":\n",
        "                    dae.fit(trloader, valoader, lr=lr, batch_size=batch_size, num_epochs=num_epochs, corrupt=corrupt, loss_type=\"cross-entropy\")\n",
        "                else:\n",
        "                    dae.fit(trloader, valoader, lr=lr, batch_size=batch_size, num_epochs=num_epochs, corrupt=corrupt, loss_type=\"mse\")\n",
        "            data_x = dae.encodeBatch(trloader)\n",
        "            valid_x = dae.encodeBatch(valoader)\n",
        "            trainset = Dataset(data_x, data_x)\n",
        "            trloader = torch.utils.data.DataLoader(\n",
        "                trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "            validset = Dataset(valid_x, valid_x)\n",
        "            valoader = torch.utils.data.DataLoader(\n",
        "                validset, batch_size=1000, shuffle=False, num_workers=2)\n",
        "            daeLayers.append(dae)\n",
        "\n",
        "        self.copyParam(daeLayers)\n",
        "\n",
        "    def copyParam(self, daeLayers):\n",
        "        if self.dropout==0:\n",
        "            every = 2\n",
        "        else:\n",
        "            every = 3\n",
        "        # input layer\n",
        "        # copy encoder weight\n",
        "        self.encoder[0].weight.data.copy_(daeLayers[0].weight.data)\n",
        "        self.encoder[0].bias.data.copy_(daeLayers[0].bias.data)\n",
        "        self._dec.weight.data.copy_(daeLayers[0].deweight.data)\n",
        "        self._dec.bias.data.copy_(daeLayers[0].vbias.data)\n",
        "\n",
        "        for l in range(1, len(self.layers)-2):\n",
        "            # copy encoder weight\n",
        "            self.encoder[l*every].weight.data.copy_(daeLayers[l].weight.data)\n",
        "            self.encoder[l*every].bias.data.copy_(daeLayers[l].bias.data)\n",
        "\n",
        "            # copy decoder weight\n",
        "            self.decoder[-(l-1)*every-2].weight.data.copy_(daeLayers[l].deweight.data)\n",
        "            self.decoder[-(l-1)*every-2].bias.data.copy_(daeLayers[l].vbias.data)\n",
        "\n",
        "        # z layer\n",
        "        self._enc_mu.weight.data.copy_(daeLayers[-1].weight.data)\n",
        "        self._enc_mu.bias.data.copy_(daeLayers[-1].bias.data)\n",
        "        self.decoder[0].weight.data.copy_(daeLayers[-1].deweight.data)\n",
        "        self.decoder[0].bias.data.copy_(daeLayers[-1].vbias.data)\n",
        "\n",
        "    def fit(self, trainloader, lr=0.001, num_epochs=10, corrupt=0.3,\n",
        "        loss_type=\"mse\"):\n",
        "        \"\"\"\n",
        "        data_x: FloatTensor\n",
        "        valid_x: FloatTensor\n",
        "        \"\"\"\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        if use_cuda:\n",
        "            self.cuda()\n",
        "        print(\"=====Stacked Denoising Autoencoding layer=======\")\n",
        "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
        "        if loss_type==\"mse\":\n",
        "            criterion = MSELoss()\n",
        "        elif loss_type==\"cross-entropy\":\n",
        "            criterion = BCELoss()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # train 1 epoch\n",
        "            train_loss = 0.0\n",
        "            for batch_idx, (inputs) in enumerate(trainloader):\n",
        "                if self.activation == \"relu\":\n",
        "                    inputs = inputs.view(inputs.size(0), -1).float()\n",
        "                inputs_corr = masking_noise(inputs, corrupt)\n",
        "                if use_cuda:\n",
        "                    inputs = inputs.cuda()\n",
        "                    inputs_corr = inputs_corr.cuda()\n",
        "                optimizer.zero_grad()\n",
        "                inputs = Variable(inputs)\n",
        "                inputs_corr = Variable(inputs_corr)\n",
        "\n",
        "                z, outputs = self.forward(inputs_corr)\n",
        "                #print(inputs_corr.shape)\n",
        "                #print(z.shape)\n",
        "                #print(outputs.shape)\n",
        "                if self.activation != \"tanh\":\n",
        "                    recon_loss = criterion(outputs, inputs)\n",
        "                else:\n",
        "                    recon_loss = criterion(outputs.reshape(inputs.shape),inputs)\n",
        "                train_loss += recon_loss.data.item()*len(inputs)\n",
        "                recon_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            print(\"#Epoch %3d: Reconstruct Loss: %.3f\" % (\n",
        "                epoch+1, train_loss / len(trainloader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78V6IImOGAv5"
      },
      "source": [
        "def do_eval(z,do_tsne=False, do_umap = True):\n",
        "    if \"tensor\" in str(type(z)):\n",
        "        latent_repr = z.detach().cpu().numpy()\n",
        "    else:\n",
        "        latent_repr = z\n",
        "    if latent_repr.shape[1] > 50:\n",
        "        print(\"Big latent representation, reducing with PCA\")\n",
        "        pca = PCA(n_components=10)\n",
        "        x_50 = pca.fit_transform(latent_repr)\n",
        "        latent_repr2 = x_50\n",
        "        print(\"Smaller latent representation, doing only PCs 1 and 2\")\n",
        "        pca = PCA(n_components=2)\n",
        "        x = pca.fit_transform(latent_repr)\n",
        "    else:\n",
        "        print(\"Smaller latent representation, doing only PCs 1 and 2\")\n",
        "        pca = PCA(n_components=2)\n",
        "        x = pca.fit_transform(latent_repr)\n",
        "        latent_repr2 = latent_repr\n",
        "    \n",
        "    \n",
        "    with open(DATA_PATH, 'r') as file:\n",
        "            reader = csv.reader(file, delimiter='\\t')\n",
        "            next(reader)\n",
        "            classes = set()\n",
        "            data = list()\n",
        "            for line in reader:\n",
        "                data.append((line[0], line[1:]))\n",
        "                classes |= set(line[0])\n",
        "    labels = [x[1][2] for x in data]\n",
        "    ag = labels\n",
        "    ag_set = set()\n",
        "    for element in ag:\n",
        "        ag_set |= {element}\n",
        "    ag_map = {name: i for i, name in enumerate(ag_set)}\n",
        "    ag_color = [ag_map[s] for s in ag]\n",
        "    \n",
        "    #tSNE\n",
        "    if do_tsne:\n",
        "        print(\"Doing tSNE\")\n",
        "        tsne = TSNE(n_components=2, verbose=1, random_state=123,n_jobs = 10)  \n",
        "        z = tsne.fit_transform(latent_repr2)\n",
        "        df = pd.DataFrame()\n",
        "        df[\"y\"] = ag\n",
        "        df[\"comp-1\"] = z[:, 0]\n",
        "        df[\"comp-2\"] = z[:, 1]\n",
        "        sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(), legend='full',\n",
        "                         palette=sns.color_palette(\"hls\", 10),\n",
        "                         data=df).set(title=\"Antigen T-SNE projection\")\n",
        "        plt.savefig(\"tSNE_output.png\")\n",
        "    \n",
        "    # UMAP\n",
        "    if do_umap:\n",
        "        print(\"Doing UMAP\")\n",
        "        ag_umap = umap.UMAP()\n",
        "        embedding = pd.DataFrame(ag_umap.fit_transform(latent_repr2),\n",
        "                                 columns=['UMAP1', 'UMAP2'])\n",
        "        sns_plot = sns.scatterplot(x='UMAP1', y='UMAP2', data=embedding, hue=np.array(ag),\n",
        "                                   alpha=.5, linewidth=0, s=1)\n",
        "        sns_plot.legend(bbox_to_anchor=(1.03, 1), fontsize=12)\n",
        "        sns_plot.figure.savefig('UMAP_output.png', bbox_inches='tight', dpi=500)\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.scatter(x[:, 0], x[:, 1], c=np.array(ag_color),\n",
        "                cmap=plt.get_cmap('tab10'), alpha=0.5)\n",
        "    plt.savefig('tab10.png')\n",
        "    \n",
        "    print(\"Doing k-means\")\n",
        "    # clustering\n",
        "    kmeans = KMeans(n_clusters=10).fit(latent_repr)\n",
        "    print(\"Doing GMM\")\n",
        "    gm = GaussianMixture(n_components=10, random_state=0).fit(latent_repr)\n",
        "    #Cluster purity\n",
        "        # computing cluster purity\n",
        "    purity = [0.0] * 10\n",
        "    occ_epis = {new_list: [0] * 10 for new_list in range(10)}\n",
        "    seq_clusters = [0] * 10\n",
        "\n",
        "    for i, j in zip(np.array(ag_color), kmeans.labels_):\n",
        "        seq_clusters[j] += 1\n",
        "        tmp = occ_epis.get(j)\n",
        "        tmp[i] += 1\n",
        "        occ_epis.update({j: tmp})\n",
        "\n",
        "    for j in range(len(seq_clusters)):\n",
        "        purity[j] = max(occ_epis[j]) / seq_clusters[j]\n",
        "    # eval clustering\n",
        "    print(\"Kmeans\")\n",
        "    print(\"Adjusted MI:\", ads(np.array(ag_color), kmeans.labels_))\n",
        "    print(\"Normalized MI:\",nis(np.array(ag_color), kmeans.labels_))\n",
        "    print(\"Purity:\", purity)\n",
        "    print(\"GMM\")\n",
        "    print(\"Adjusted MI:\", ads(np.array(ag_color), gm.predict(latent_repr)))\n",
        "    print(\"Normalized MI:\",nis(np.array(ag_color),  gm.predict(latent_repr)))\n",
        "    print(\"Purity:\", purity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL5t-eewGA0c"
      },
      "source": [
        "#Training variables\n",
        "DATA_PATH = 'hackathon.csv'\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 5000\n",
        "LATENT_N = 32 #Bigg.\n",
        "DEVICE = 'cuda:0'  # change to 'cpu' if training on cpu\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#Load data\n",
        "dataset = CDRH3MotifDataset(DATA_PATH, device=DEVICE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSq7xuhXGA2n"
      },
      "source": [
        "with open(DATA_PATH, 'r') as file:\n",
        "    reader = csv.reader(file, delimiter='\\t')\n",
        "    next(reader)\n",
        "    classes = set()\n",
        "    data = list()\n",
        "    for line in reader:\n",
        "        data.append((line[0], line[1:]))\n",
        "        classes |= set(line[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3BHUQnyGA4w"
      },
      "source": [
        "peptides = [x[0] for x in data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o55fyCFPGA6o"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "os.getcwd()\n",
        "model_dir = Path(os.path.join(os.getcwd(),\"uniref50_v2\"))\n",
        "weights = model_dir / 'weights.hdf5'\n",
        "options = model_dir / 'options.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQxoSPIEGA8q"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from allennlp.commands.elmo import ElmoEmbedder\n",
        "\n",
        "embedder = ElmoEmbedder(options,weights, cuda_device=0) # cuda_device=-1 for CPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVBpEO33ILU0"
      },
      "source": [
        "peptideslist = [list(x) for x in peptides]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ha1syo9ILqJ"
      },
      "source": [
        "peptideslist.sort(key=len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMa6c3NNIL7v"
      },
      "source": [
        "embedding = embedder.embed_sentences(peptideslist) # returns: List-of-Lists with shape [3,L,1024]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWEmQVqSIL-P"
      },
      "source": [
        "my_tensor = torch.ones((len(dataset),11,1024), dtype=torch.float, device=DEVICE, requires_grad=False)\n",
        "for i in enumerate(embedding):\n",
        "    temp_tensor = i[1].sum(0)\n",
        "    my_tensor[i[0],:,:] = torch.Tensor(temp_tensor).to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ2JZszCNzBe"
      },
      "source": [
        "data = DataLoader(my_tensor, batch_size=BATCH_SIZE, shuffle=True, num_workers=0,\n",
        "                  drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXktuC7lOBak"
      },
      "source": [
        "for i in enumerate(data):\n",
        "    temp_tensor = i[1]\n",
        "    print(temp_tensor.shape)\n",
        "    feature_shape = temp_tensor.shape[1]*temp_tensor.shape[2]\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cdYnW00OD2V"
      },
      "source": [
        "for i in [(1200,1200),(600,600),(500,500),(400,400),(300,300),(200,200),(100,100)]:\n",
        "    break\n",
        "    SDAE = StackedDAE(feature_shape,LATENT_N,False,[i[0]],[i[1]],dropout=0.0)\n",
        "    SDAE.fit(data,num_epochs=EPOCHS,corrupt=0.1)\n",
        "\n",
        "    data = DataLoader(my_tensor, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n",
        "                    drop_last = False)\n",
        "\n",
        "    all_z = np.zeros((len(dataset),LATENT_N),np.float32)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs) in enumerate(data):\n",
        "            inputs = inputs.view(inputs.size(0), -1).float()\n",
        "            z, outputs = SDAE.forward(inputs)\n",
        "            if z.shape[0] == BATCH_SIZE:\n",
        "                all_z[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE,:] = z.cpu().numpy()\n",
        "            else:\n",
        "                all_z[batch_idx*BATCH_SIZE:len(dataset),:] = z.cpu().numpy()\n",
        "\n",
        "    do_eval(all_z,do_umap = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU2yIKVYf4Kf"
      },
      "source": [
        "feature_shape = [11,1024]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR26kyJ0Ufs6"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "#LSTM autoencoder\n",
        "for i in [(300,300),(200,200),(100,100)]:\n",
        "\n",
        "    SDAE = StackedDAE(feature_shape,LATENT_N,False,[i[0]//100],[i[1]//100],dropout=0.5,\n",
        "                      activation=\"tanh\")\n",
        "    SDAE.fit(data,num_epochs=EPOCHS,corrupt=0.4)\n",
        "\n",
        "    data = DataLoader(my_tensor, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n",
        "                    drop_last = False)\n",
        "\n",
        "    all_z = np.zeros((len(dataset),LATENT_N),np.float32)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs) in enumerate(data):\n",
        "            #inputs = inputs.view(inputs.size(0), -1).float()\n",
        "            z, outputs = SDAE.forward(inputs)\n",
        "            if z.shape[0] == BATCH_SIZE:\n",
        "                all_z[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE,:] = z.cpu().numpy()\n",
        "            else:\n",
        "                all_z[batch_idx*BATCH_SIZE:len(dataset),:] = z.cpu().numpy()\n",
        "\n",
        "    do_eval(all_z,do_umap = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvBvbkjN8IdC"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "#LSTM autoencoder\n",
        "for i in [(300,300),(200,200),(100,100)]:\n",
        "\n",
        "    SDAE = StackedDAE(feature_shape,LATENT_N,False,[i[0]//100],[i[1]//100],dropout=0.5,\n",
        "                      activation=\"tanh\")\n",
        "    SDAE.fit(data,num_epochs=EPOCHS,corrupt=0.4)\n",
        "\n",
        "    data = DataLoader(my_tensor, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n",
        "                    drop_last = False)\n",
        "\n",
        "    all_z = np.zeros((len(dataset),LATENT_N),np.float32)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs) in enumerate(data):\n",
        "            #inputs = inputs.view(inputs.size(0), -1).float()\n",
        "            z, outputs = SDAE.forward(inputs)\n",
        "            if z.shape[0] == BATCH_SIZE:\n",
        "                all_z[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE,:] = z.cpu().numpy()\n",
        "            else:\n",
        "                all_z[batch_idx*BATCH_SIZE:len(dataset),:] = z.cpu().numpy()\n",
        "\n",
        "    do_eval(all_z,do_umap = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe-74lPq-RsW"
      },
      "source": [
        "#Moving onto stacked DAE\n",
        "\n",
        "def buildNetwork2(layers, activation=\"relu\", dropout=0,embedding = False):\n",
        "    net = []\n",
        "    print(layers)\n",
        "    net.append(nn.Embedding(20,64))\n",
        "    for i in range(1, len(layers)):\n",
        "        if activation==\"relu\":\n",
        "            net.append(nn.Linear(64, 64))\n",
        "            net.append(nn.ReLU())\n",
        "        elif activation==\"tanh\":\n",
        "            new_input_dim = tuple([BATCH_SIZE] + layers[0])\n",
        "            net.append(nn.GRU(input_size = 64,hidden_size = 32,\n",
        "                              num_layers = layers[1],batch_first = True,dropout = dropout))\n",
        "            break\n",
        "        elif activation==\"sigmoid\":\n",
        "            net.append(nn.Sigmoid())\n",
        "        if activation != \"tanh\":\n",
        "            if dropout > 0:\n",
        "                net.append(nn.Dropout(dropout))\n",
        "    return nn.Sequential(*net)\n",
        "\n",
        "def buildNetwork3(layers, activation=\"relu\", dropout=0,embedding = True):\n",
        "    net = []\n",
        "    print(layers)\n",
        "    for i in range(1, len(layers)):\n",
        "        if activation==\"relu\":\n",
        "            net.append(nn.Linear(layers[i-1], layers[i][0]))\n",
        "            net.append(nn.ReLU())\n",
        "        elif activation==\"tanh\":\n",
        "            new_input_dim = tuple([BATCH_SIZE] + layers[0])\n",
        "            net.append(nn.GRU(input_size = 64,hidden_size = 32,\n",
        "                              num_layers = layers[1],batch_first = True,dropout = dropout))\n",
        "            break\n",
        "        elif activation==\"sigmoid\":\n",
        "            net.append(nn.Sigmoid())\n",
        "        if activation != \"tanh\":\n",
        "            if dropout > 0:\n",
        "                net.append(nn.Dropout(dropout))\n",
        "    return nn.Sequential(*net)\n",
        "\n",
        "class StackedDAE2(nn.Module):\n",
        "    def __init__(self, input_dim=784, z_dim=10, binary=True,\n",
        "        encodeLayer=[400], decodeLayer=[400], activation=\"relu\", \n",
        "        dropout=0, tied=False,embedding = False):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.layers = [input_dim] + encodeLayer + [z_dim]\n",
        "        self.activation = activation\n",
        "        self.dropout = dropout\n",
        "        self.embedding = embedding\n",
        "        self.encoder = buildNetwork2([input_dim] + encodeLayer, activation=activation, dropout=dropout,\n",
        "                                    embedding = self.embedding)\n",
        "        if activation==\"tanh\":\n",
        "            self.decoder = buildNetwork2([[z_dim,z_dim]] + decodeLayer, activation=activation, dropout=dropout,\n",
        "                                        embedding = self.embedding)\n",
        "        else:\n",
        "            self.decoder = buildNetwork3([z_dim] + [decodeLayer], activation=activation, dropout=dropout,\n",
        "                                         embedding = self.embedding)\n",
        "        if self.activation != \"tanh\":\n",
        "            self._enc_mu = nn.Linear(11*20*64, z_dim)\n",
        "            print(decodeLayer[-1])\n",
        "            self._dec = nn.Linear(decodeLayer[-1], input_dim[0]*input_dim[1])\n",
        "            self._dec_act = None\n",
        "        else:\n",
        "            self._enc_mu = nn.Linear(32*encodeLayer[-1],z_dim)\n",
        "            print(input_dim[0], input_dim[1],input_dim[0]*input_dim[1])\n",
        "            self._dec = nn.Linear(32,input_dim[0]*input_dim[1])\n",
        "            self._dec_act = None\n",
        "        if binary:\n",
        "            self._dec_act = nn.Sigmoid()\n",
        "\n",
        "    def decode(self, z):\n",
        "        if self.activation != \"tanh\":\n",
        "            h = self.decoder(z)\n",
        "            x = self._dec(h)\n",
        "        else:\n",
        "            z_new = z.unsqueeze(1)\n",
        "            #print(\"decoder z shape\",z_new.shape)\n",
        "            h, h_n = self.decoder(z_new)\n",
        "            h_new = h.reshape((h.shape[0],-1))\n",
        "            #print(\"hnew shape\",h_new.shape)\n",
        "            x = self._dec(h_new)\n",
        "            #print(\"x shape\",x.shape)\n",
        "        if self._dec_act is not None:\n",
        "            x = self._dec_act(x)\n",
        "        return x\n",
        "\n",
        "    def loss_function(self, recon_x, x):\n",
        "        loss = -torch.mean(torch.sum(x*torch.log(torch.clamp(recon_x, min=1e-10))+\n",
        "            (1-x)*torch.log(torch.clamp(1-recon_x, min=1e-10)), 1))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        if(self.activation != \"tanh\"):\n",
        "            #print(\"x shape\",x.shape)\n",
        "            h = self.encoder(x)\n",
        "            #print(\"h shape\",h.shape)\n",
        "            h_new = h.reshape(h.shape[0],-1)\n",
        "            #print(\"h new hspae\",h_new.shape)\n",
        "            z = self._enc_mu(h_new)\n",
        "        else:\n",
        "            h,h_n = self.encoder(x)\n",
        "            h_new = h.reshape((h.shape[0],-1))\n",
        "            #print(\"hnew shape\",h_new.shape)\n",
        "            z = self._enc_mu(h_new)\n",
        "            #print(\"z shape\",z.shape)\n",
        "        return z, self.decode(z)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
        "        model_dict = self.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict) \n",
        "        self.load_state_dict(model_dict)\n",
        "\n",
        "    def pretrain(self, trainloader, validloader, lr=0.001, batch_size=128, num_epochs=10, corrupt=0.3, loss_type=\"cross-entropy\"):\n",
        "        trloader = trainloader\n",
        "        valoader = validloader\n",
        "        daeLayers = []\n",
        "        for l in range(1, len(self.layers)):\n",
        "            infeatures = self.layers[l-1]\n",
        "            outfeatures = self.layers[l]\n",
        "            dae = DenoisingAutoencoder(infeatures, outfeatures, activation=self.activation, dropout=self.dropout)\n",
        "            if l==1:\n",
        "                dae.fit(trloader, valoader, lr=lr, batch_size=batch_size, num_epochs=num_epochs, corrupt=corrupt, loss_type=loss_type)\n",
        "            else:\n",
        "                if self.activation==\"sigmoid\":\n",
        "                    dae.fit(trloader, valoader, lr=lr, batch_size=batch_size, num_epochs=num_epochs, corrupt=corrupt, loss_type=\"cross-entropy\")\n",
        "                else:\n",
        "                    dae.fit(trloader, valoader, lr=lr, batch_size=batch_size, num_epochs=num_epochs, corrupt=corrupt, loss_type=\"mse\")\n",
        "            data_x = dae.encodeBatch(trloader)\n",
        "            valid_x = dae.encodeBatch(valoader)\n",
        "            trainset = Dataset(data_x, data_x)\n",
        "            trloader = torch.utils.data.DataLoader(\n",
        "                trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "            validset = Dataset(valid_x, valid_x)\n",
        "            valoader = torch.utils.data.DataLoader(\n",
        "                validset, batch_size=1000, shuffle=False, num_workers=2)\n",
        "            daeLayers.append(dae)\n",
        "\n",
        "        self.copyParam(daeLayers)\n",
        "\n",
        "    def copyParam(self, daeLayers):\n",
        "        if self.dropout==0:\n",
        "            every = 2\n",
        "        else:\n",
        "            every = 3\n",
        "        # input layer\n",
        "        # copy encoder weight\n",
        "        self.encoder[0].weight.data.copy_(daeLayers[0].weight.data)\n",
        "        self.encoder[0].bias.data.copy_(daeLayers[0].bias.data)\n",
        "        self._dec.weight.data.copy_(daeLayers[0].deweight.data)\n",
        "        self._dec.bias.data.copy_(daeLayers[0].vbias.data)\n",
        "\n",
        "        for l in range(1, len(self.layers)-2):\n",
        "            # copy encoder weight\n",
        "            self.encoder[l*every].weight.data.copy_(daeLayers[l].weight.data)\n",
        "            self.encoder[l*every].bias.data.copy_(daeLayers[l].bias.data)\n",
        "\n",
        "            # copy decoder weight\n",
        "            self.decoder[-(l-1)*every-2].weight.data.copy_(daeLayers[l].deweight.data)\n",
        "            self.decoder[-(l-1)*every-2].bias.data.copy_(daeLayers[l].vbias.data)\n",
        "\n",
        "        # z layer\n",
        "        self._enc_mu.weight.data.copy_(daeLayers[-1].weight.data)\n",
        "        self._enc_mu.bias.data.copy_(daeLayers[-1].bias.data)\n",
        "        self.decoder[0].weight.data.copy_(daeLayers[-1].deweight.data)\n",
        "        self.decoder[0].bias.data.copy_(daeLayers[-1].vbias.data)\n",
        "\n",
        "    def fit(self, trainloader, lr=0.001, num_epochs=10, corrupt=0.3,\n",
        "        loss_type=\"mse\"):\n",
        "        \"\"\"\n",
        "        data_x: FloatTensor\n",
        "        valid_x: FloatTensor\n",
        "        \"\"\"\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        if use_cuda:\n",
        "            self.cuda()\n",
        "        print(\"=====Stacked Denoising Autoencoding layer=======\")\n",
        "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
        "        if loss_type==\"mse\":\n",
        "            criterion = MSELoss()\n",
        "        elif loss_type==\"cross-entropy\":\n",
        "            criterion = BCELoss()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # train 1 epoch\n",
        "            train_loss = 0.0\n",
        "            for batch_idx, (inputs) in enumerate(trainloader):\n",
        "                if self.activation == \"relu\":\n",
        "                    pass\n",
        "                    #inputs = inputs.view(inputs.size(0), -1).float()\n",
        "                inputs_corr = masking_noise(inputs, corrupt)\n",
        "                if use_cuda:\n",
        "                    inputs = inputs.cuda()\n",
        "                    inputs_corr = inputs_corr.cuda()\n",
        "                optimizer.zero_grad()\n",
        "                inputs = Variable(inputs)\n",
        "                inputs_corr = Variable(inputs_corr)\n",
        "\n",
        "                z, outputs = self.forward(inputs_corr)\n",
        "                #print(inputs_corr.shape)\n",
        "                #print(z.shape)\n",
        "                #print(outputs.shape)\n",
        "                outputs = outputs.reshape(inputs.shape)\n",
        "                if self.activation != \"tanh\":\n",
        "                    recon_loss = criterion(outputs, inputs)\n",
        "                else:\n",
        "                    recon_loss = criterion(outputs.reshape(inputs.shape),inputs)\n",
        "                train_loss += recon_loss.data.item()*len(inputs)\n",
        "                recon_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            print(\"#Epoch %3d: Reconstruct Loss: %.3f\" % (\n",
        "                epoch+1, train_loss / len(trainloader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnb60nIE_Scg"
      },
      "source": [
        "data = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0,\n",
        "                  drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVZ7PSx1DCnM"
      },
      "source": [
        "for i in data:\n",
        "    print(i)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AVtbMkB-9jh"
      },
      "source": [
        "for i in [(1200,1200),(600,600),(500,500),(400,400),(300,300),(200,200),(100,100)]:\n",
        "    SDAE = StackedDAE2([11,20],LATENT_N,True,[i[0]],[i[1]],dropout=0.0)\n",
        "    SDAE.fit(data,num_epochs=EPOCHS,corrupt=0.1)\n",
        "\n",
        "    data = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n",
        "                    drop_last = False)\n",
        "\n",
        "    all_z = np.zeros((len(dataset),LATENT_N),np.float32)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs) in enumerate(data):\n",
        "            #inputs = inputs.view(inputs.size(0), -1).float()\n",
        "            z, outputs = SDAE.forward(inputs)\n",
        "            if z.shape[0] == BATCH_SIZE:\n",
        "                all_z[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE,:] = z.cpu().numpy()\n",
        "            else:\n",
        "                all_z[batch_idx*BATCH_SIZE:len(dataset),:] = z.cpu().numpy()\n",
        "\n",
        "    do_eval(all_z,do_umap = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j7yxuDDHnWk"
      },
      "source": [
        "for i in [(1200,1200),(600,600),(500,500),(400,400),(300,300),(200,200),(100,100)]:\n",
        "    SDAE = StackedDAE2([11,20],LATENT_N,True,[i[0]],[i[1]],dropout=0.4)\n",
        "    SDAE.fit(data,num_epochs=EPOCHS,corrupt=0.5)\n",
        "\n",
        "    data = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n",
        "                    drop_last = False)\n",
        "\n",
        "    all_z = np.zeros((len(dataset),LATENT_N),np.float32)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs) in enumerate(data):\n",
        "            #inputs = inputs.view(inputs.size(0), -1).float()\n",
        "            z, outputs = SDAE.forward(inputs)\n",
        "            if z.shape[0] == BATCH_SIZE:\n",
        "                all_z[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE,:] = z.cpu().numpy()\n",
        "            else:\n",
        "                all_z[batch_idx*BATCH_SIZE:len(dataset),:] = z.cpu().numpy()\n",
        "\n",
        "    do_eval(all_z,do_umap = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg3_u5NAKU_y"
      },
      "source": [
        "LATENT_N = 32\n",
        "for i in [(1200,1200),(600,600),(500,500),(400,400),(300,300),(200,200),(100,100)]:\n",
        "    SDAE = StackedDAE2([11,20],LATENT_N,True,[i[0]],[i[1]],dropout=0.4)\n",
        "    SDAE.fit(data,num_epochs=EPOCHS,corrupt=0.5)\n",
        "\n",
        "    data = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n",
        "                    drop_last = False)\n",
        "\n",
        "    all_z = np.zeros((len(dataset),LATENT_N),np.float32)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs) in enumerate(data):\n",
        "            #inputs = inputs.view(inputs.size(0), -1).float()\n",
        "            z, outputs = SDAE.forward(inputs)\n",
        "            if z.shape[0] == BATCH_SIZE:\n",
        "                all_z[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE,:] = z.cpu().numpy()\n",
        "            else:\n",
        "                all_z[batch_idx*BATCH_SIZE:len(dataset),:] = z.cpu().numpy()\n",
        "\n",
        "    do_eval(all_z,do_umap = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mv7--hXMq5x"
      },
      "source": [
        "LATENT_N = 16\n",
        "for i in [(1200,1200),(600,600),(500,500),(400,400),(300,300),(200,200),(100,100)]:\n",
        "    SDAE = StackedDAE2([11,20],LATENT_N,True,[i[0]],[i[1]],dropout=0.4)\n",
        "    SDAE.fit(data,num_epochs=EPOCHS,corrupt=0.5)\n",
        "\n",
        "    data = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n",
        "                    drop_last = False)\n",
        "\n",
        "    all_z = np.zeros((len(dataset),LATENT_N),np.float32)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs) in enumerate(data):\n",
        "            #inputs = inputs.view(inputs.size(0), -1).float()\n",
        "            z, outputs = SDAE.forward(inputs)\n",
        "            if z.shape[0] == BATCH_SIZE:\n",
        "                all_z[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE,:] = z.cpu().numpy()\n",
        "            else:\n",
        "                all_z[batch_idx*BATCH_SIZE:len(dataset),:] = z.cpu().numpy()\n",
        "\n",
        "    do_eval(all_z,do_umap = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n_fuErsPrIF"
      },
      "source": [
        "LATENT_N = 64\n",
        "for i in [(1200,1200),(600,600),(500,500),(400,400),(300,300),(200,200),(100,100)]:\n",
        "    SDAE = StackedDAE2([11,20],LATENT_N,True,[i[0]],[i[1]],dropout=0.3)\n",
        "    SDAE.fit(data,num_epochs=EPOCHS,corrupt=0.3)\n",
        "\n",
        "    data = DataLoader(dataset, batch_size=BATCH_SbIZE, shuffle=False, num_workers=0,\n",
        "                    drop_last = False)\n",
        "\n",
        "    all_z = np.zeros((len(dataset),LATENT_N),np.float32)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs) in enumerate(data):\n",
        "            #inputs = inputs.view(inputs.size(0), -1).float()\n",
        "            z, outputs = SDAE.forward(inputs)\n",
        "            if z.shape[0] == BATCH_SIZE:\n",
        "                all_z[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE,:] = z.cpu().numpy()\n",
        "            else:\n",
        "                all_z[batch_idx*BATCH_SIZE:len(dataset),:] = z.cpu().numpy()\n",
        "\n",
        "    do_eval(all_z,do_umap = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m6CXB9YSA2z"
      },
      "source": [
        "!pip install gpflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "772Et77bSCNO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "def cluster_acc(Y_pred, Y):\n",
        "  from sklearn.utils.linear_assignment_ import linear_assignment\n",
        "  assert Y_pred.size == Y.size\n",
        "  D = max(Y_pred.max(), Y.max())+1\n",
        "  D = int(D)\n",
        "  w = np.zeros((D,D), dtype=np.float64)\n",
        "  for i in range(Y_pred.size):\n",
        "    Y = [int(x) for x in Y]\n",
        "    w[Y_pred[i], Y[i]] += 1\n",
        "  ind = linear_assignment(w.max() - w)\n",
        "  return sum([w[i,j] for i,j in ind])*1.0/Y_pred.size, w\n",
        "\n",
        "def buildNetwork(layers, activation=\"relu\", dropout=0):\n",
        "    net = []\n",
        "    for i in range(1, len(layers)):\n",
        "        net.append(nn.Linear(layers[i-1], layers[i]))\n",
        "        if activation==\"relu\":\n",
        "            net.append(nn.ReLU())\n",
        "        elif activation==\"sigmoid\":\n",
        "            net.append(nn.Sigmoid())\n",
        "        if dropout > 0:\n",
        "            net.append(nn.Dropout(dropout))\n",
        "    return nn.Sequential(*net)\n",
        "\n",
        "def adjust_learning_rate(init_lr, optimizer, epoch):\n",
        "    lr = max(init_lr * (0.9 ** (epoch//10)), 0.0002)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "    return lr\n",
        "\n",
        "log2pi = math.log(2*math.pi)\n",
        "def log_likelihood_samples_unit_gaussian(samples):\n",
        "    return -0.5*log2pi*samples.size()[1] - torch.sum(0.5*(samples)**2, 1)\n",
        "\n",
        "def log_likelihood_samplesImean_sigma(samples, mu, logvar):\n",
        "    return -0.5*log2pi*samples.size()[1] - torch.sum(0.5*(samples-mu)**2/torch.exp(logvar) + 0.5*logvar, 1)\n",
        "\n",
        "class VaDE(nn.Module):\n",
        "    def __init__(self, input_dim=784, z_dim=10, n_centroids=10, binary=True,\n",
        "        encodeLayer=[500,500,2000], decodeLayer=[2000,500,500]):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.n_centroids = n_centroids\n",
        "        self.encoder = buildNetwork([input_dim] + encodeLayer)\n",
        "        self.decoder = buildNetwork([z_dim] + decodeLayer)\n",
        "        self._enc_mu = nn.Linear(encodeLayer[-1], z_dim)\n",
        "        self._enc_log_sigma = nn.Linear(encodeLayer[-1], z_dim)\n",
        "        self._dec = nn.Linear(decodeLayer[-1], input_dim)\n",
        "        self._dec_act = None\n",
        "        if binary:\n",
        "            self._dec_act = nn.Sigmoid()\n",
        "\n",
        "        self.create_gmmparam(n_centroids, z_dim)\n",
        "\n",
        "    def create_gmmparam(self, n_centroids, z_dim):\n",
        "        self.theta_p = nn.Parameter(torch.ones(n_centroids)/n_centroids)\n",
        "        self.u_p = nn.Parameter(torch.zeros(z_dim, n_centroids))\n",
        "        self.lambda_p = nn.Parameter(torch.ones(z_dim, n_centroids))\n",
        "\n",
        "    def initialize_gmm(self, dataloader):\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        if use_cuda:\n",
        "            self.cuda()\n",
        "\n",
        "        self.eval()\n",
        "        data = []\n",
        "        for batch_idx, (inputs,labels) in enumerate(dataloader):\n",
        "            inputs = inputs.view(inputs.size(0), -1).float()\n",
        "            if use_cuda:\n",
        "                inputs = inputs.cuda()\n",
        "            inputs = Variable(inputs)\n",
        "            z, outputs, mu, logvar = self.forward(inputs)\n",
        "            data.append(z.data.cpu().numpy())\n",
        "        data = np.concatenate(data)\n",
        "        gmm = GaussianMixture(n_components=self.n_centroids,covariance_type='diag')\n",
        "        gmm.fit(data)\n",
        "        self.u_p.data.copy_(torch.from_numpy(gmm.means_.T.astype(np.float32)))\n",
        "        self.lambda_p.data.copy_(torch.from_numpy(gmm.covariances_.T.astype(np.float32)))\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "          std = logvar.mul(0.5).exp_()\n",
        "          eps = Variable(std.data.new(std.size()).normal_())\n",
        "          # num = np.array([[ 1.096506  ,  0.3686553 , -0.43172026,  1.27677995,  1.26733758,\n",
        "          #       1.30626082,  0.14179629,  0.58619505, -0.76423112,  2.67965817]], dtype=np.float32)\n",
        "          # num = np.repeat(num, mu.size()[0], axis=0)\n",
        "          # eps = Variable(torch.from_numpy(num))\n",
        "          return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "          return mu\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.decoder(z)\n",
        "        x = self._dec(h)\n",
        "        if self._dec_act is not None:\n",
        "            x = self._dec_act(x)\n",
        "        return x\n",
        "\n",
        "    def get_gamma(self, z, z_mean, z_log_var):\n",
        "        Z = z.unsqueeze(2).expand(z.size()[0], z.size()[1], self.n_centroids) # NxDxK\n",
        "        z_mean_t = z_mean.unsqueeze(2).expand(z_mean.size()[0], z_mean.size()[1], self.n_centroids)\n",
        "        z_log_var_t = z_log_var.unsqueeze(2).expand(z_log_var.size()[0], z_log_var.size()[1], self.n_centroids)\n",
        "        u_tensor3 = self.u_p.unsqueeze(0).expand(z.size()[0], self.u_p.size()[0], self.u_p.size()[1]) # NxDxK\n",
        "        lambda_tensor3 = self.lambda_p.unsqueeze(0).expand(z.size()[0], self.lambda_p.size()[0], self.lambda_p.size()[1])\n",
        "        theta_tensor2 = self.theta_p.unsqueeze(0).expand(z.size()[0], self.n_centroids) # NxK\n",
        "\n",
        "        p_c_z = torch.exp(torch.log(theta_tensor2) - torch.sum(0.5*torch.log(2*math.pi*lambda_tensor3)+\\\n",
        "            (Z-u_tensor3)**2/(2*lambda_tensor3), dim=1)) + 1e-10 # NxK\n",
        "        gamma = p_c_z / torch.sum(p_c_z, dim=1, keepdim=True)\n",
        "\n",
        "        return gamma\n",
        "\n",
        "    def loss_function(self, recon_x, x, z, z_mean, z_log_var):\n",
        "        Z = z.unsqueeze(2).expand(z.size()[0], z.size()[1], self.n_centroids) # NxDxK\n",
        "        z_mean_t = z_mean.unsqueeze(2).expand(z_mean.size()[0], z_mean.size()[1], self.n_centroids)\n",
        "        z_log_var_t = z_log_var.unsqueeze(2).expand(z_log_var.size()[0], z_log_var.size()[1], self.n_centroids)\n",
        "        u_tensor3 = self.u_p.unsqueeze(0).expand(z.size()[0], self.u_p.size()[0], self.u_p.size()[1]) # NxDxK\n",
        "        lambda_tensor3 = self.lambda_p.unsqueeze(0).expand(z.size()[0], self.lambda_p.size()[0], self.lambda_p.size()[1])\n",
        "        theta_tensor2 = self.theta_p.unsqueeze(0).expand(z.size()[0], self.n_centroids) # NxK\n",
        "        \n",
        "        p_c_z = torch.exp(torch.log(theta_tensor2) - torch.sum(0.5*torch.log(2*math.pi*lambda_tensor3)+\\\n",
        "            (Z-u_tensor3)**2/(2*lambda_tensor3), dim=1)) + 1e-10 # NxK\n",
        "        gamma = p_c_z / torch.sum(p_c_z, dim=1, keepdim=True) # NxK\n",
        "        \n",
        "        BCE = -torch.sum(x*torch.log(torch.clamp(recon_x, min=1e-10))+\n",
        "            (1-x)*torch.log(torch.clamp(1-recon_x, min=1e-10)), 1)\n",
        "        logpzc = torch.sum(0.5*gamma*torch.sum(math.log(2*math.pi)+torch.log(lambda_tensor3)+\\\n",
        "            torch.exp(z_log_var_t)/lambda_tensor3 + (z_mean_t-u_tensor3)**2/lambda_tensor3, dim=1), dim=1)\n",
        "        qentropy = -0.5*torch.sum(1+z_log_var+math.log(2*math.pi), 1)\n",
        "        logpc = -torch.sum(torch.log(theta_tensor2)*gamma, 1)\n",
        "        logqcx = torch.sum(torch.log(gamma)*gamma, 1)\n",
        "\n",
        "        # Normalise by same number of elements as in reconstruction\n",
        "        loss = torch.mean(BCE + logpzc + qentropy + logpc + logqcx)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    #===============================================================\n",
        "    # below is defined according to the released code by the authors\n",
        "    # However, they are incorrect in several places\n",
        "    #===============================================================\n",
        "\n",
        "    # def get_gamma(self, z, z_mean, z_log_var):\n",
        "    #     Z = z.unsqueeze(2).expand(z.size()[0], z.size()[1], self.n_centroids) # NxDxK\n",
        "    #     z_mean_t = z_mean.unsqueeze(2).expand(z_mean.size()[0], z_mean.size()[1], self.n_centroids)\n",
        "    #     z_log_var_t = z_log_var.unsqueeze(2).expand(z_log_var.size()[0], z_log_var.size()[1], self.n_centroids)\n",
        "    #     u_tensor3 = self.u_p.unsqueeze(0).expand(z.size()[0], self.u_p.size()[0], self.u_p.size()[1]) # NxDxK\n",
        "    #     lambda_tensor3 = self.lambda_p.unsqueeze(0).expand(z.size()[0], self.lambda_p.size()[0], self.lambda_p.size()[1])\n",
        "    #     theta_tensor3 = self.theta_p.unsqueeze(0).unsqueeze(1).expand(z.size()[0], z.size()[1], self.n_centroids) # NxDxK\n",
        "\n",
        "    #     p_c_z = torch.exp(torch.sum(torch.log(theta_tensor3) - 0.5*torch.log(2*math.pi*lambda_tensor3)-\\\n",
        "    #         (Z-u_tensor3)**2/(2*lambda_tensor3), dim=1)) + 1e-10 # NxK\n",
        "    #     gamma = p_c_z / torch.sum(p_c_z, dim=1, keepdim=True) # NxK\n",
        "\n",
        "    #     return gamma\n",
        "\n",
        "    # def loss_function(self, recon_x, x, z, z_mean, z_log_var):\n",
        "    #     Z = z.unsqueeze(2).expand(z.size()[0], z.size()[1], self.n_centroids) # NxDxK\n",
        "    #     z_mean_t = z_mean.unsqueeze(2).expand(z_mean.size()[0], z_mean.size()[1], self.n_centroids)\n",
        "    #     z_log_var_t = z_log_var.unsqueeze(2).expand(z_log_var.size()[0], z_log_var.size()[1], self.n_centroids)\n",
        "    #     u_tensor3 = self.u_p.unsqueeze(0).expand(z.size()[0], self.u_p.size()[0], self.u_p.size()[1]) # NxDxK\n",
        "    #     lambda_tensor3 = self.lambda_p.unsqueeze(0).expand(z.size()[0], self.lambda_p.size()[0], self.lambda_p.size()[1])\n",
        "    #     theta_tensor3 = self.theta_p.unsqueeze(0).unsqueeze(1).expand(z.size()[0], z.size()[1], self.n_centroids) # NxDxK\n",
        "\n",
        "    #     p_c_z = torch.exp(torch.sum(torch.log(theta_tensor3) - 0.5*torch.log(2*math.pi*lambda_tensor3)-\\\n",
        "    #         (Z-u_tensor3)**2/(2*lambda_tensor3), dim=1)) + 1e-10 # NxK\n",
        "    #     gamma = p_c_z / torch.sum(p_c_z, dim=1, keepdim=True) # NxK\n",
        "    #     gamma_t = gamma.unsqueeze(1).expand(gamma.size(0), self.z_dim, gamma.size(1)) #\n",
        "        \n",
        "    #     BCE = -torch.sum(x*torch.log(torch.clamp(recon_x, min=1e-10))+\n",
        "    #         (1-x)*torch.log(torch.clamp(1-recon_x, min=1e-10)), 1)\n",
        "    #     logpzc = torch.sum(torch.sum(0.5*gamma_t*(self.z_dim*math.log(2*math.pi)+torch.log(lambda_tensor3)+\\\n",
        "    #         torch.exp(z_log_var_t)/lambda_tensor3 + (z_mean_t-u_tensor3)**2/lambda_tensor3), dim=1), dim=1)\n",
        "    #     qentropy = -0.5*torch.sum(1+z_log_var+math.log(2*math.pi), 1)\n",
        "    #     logpc = -torch.sum(torch.log(self.theta_p.unsqueeze(0).expand(z.size()[0], self.n_centroids))*gamma, 1)\n",
        "    #     logqcx = torch.sum(torch.log(gamma)*gamma, 1)\n",
        "\n",
        "    #     loss = torch.mean(BCE + logpzc + qentropy + logpc + logqcx)\n",
        "\n",
        "    #     # return torch.mean(qentropy)\n",
        "    #     return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self._enc_mu(h)\n",
        "        logvar = self._enc_log_sigma(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return z, self.decode(z), mu, logvar\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
        "        model_dict = self.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict) \n",
        "        self.load_state_dict(model_dict)\n",
        "\n",
        "    def fit(self, trainloader, validloader, lr=0.001, batch_size=128, num_epochs=10, \n",
        "        visualize=False, anneal=False):\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        if use_cuda:\n",
        "            self.cuda()\n",
        "\n",
        "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
        "\n",
        "        # validate\n",
        "        self.eval()\n",
        "        valid_loss = 0.0\n",
        "        for batch_idx, (inputs,labels) in enumerate(validloader):\n",
        "            inputs = inputs.view(inputs.size(0), -1).float()\n",
        "            if use_cuda:\n",
        "                inputs = inputs.cuda()\n",
        "            inputs = Variable(inputs)\n",
        "            z, outputs, mu, logvar = self.forward(inputs)\n",
        "\n",
        "            loss = self.loss_function(outputs, inputs, z, mu, logvar)\n",
        "            valid_loss += loss.data.item()*len(inputs)\n",
        "            # total_loss += valid_recon_loss.data[0] * inputs.size()[0]\n",
        "            # total_num += inputs.size()[0]\n",
        "\n",
        "        # valid_loss = total_loss / total_num\n",
        "        print(\"#Epoch -1: Valid Loss: %.5f\" % (valid_loss / len(validloader.dataset)))\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # train 1 epoch\n",
        "            self.train()\n",
        "            if anneal:\n",
        "                epoch_lr = adjust_learning_rate(lr, optimizer, epoch)\n",
        "            train_loss = 0\n",
        "            for batch_idx, (inputs) in enumerate(trainloader):\n",
        "                #print(\"Train batch idx\", batch_idx)\n",
        "                inputs = inputs.view(inputs.size(0), -1).float()\n",
        "                if use_cuda:\n",
        "                    inputs = inputs.cuda()\n",
        "                optimizer.zero_grad()\n",
        "                inputs = Variable(inputs)\n",
        "                \n",
        "                z, outputs, mu, logvar = self.forward(inputs)\n",
        "                loss = self.loss_function(outputs, inputs, z, mu, logvar)\n",
        "                train_loss += loss.data.item()*len(inputs)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                # print(\"    #Iter %3d: Reconstruct Loss: %.3f\" % (\n",
        "                #     batch_idx, recon_loss.data[0]))\n",
        "\n",
        "            # validate\n",
        "            self.eval()\n",
        "            valid_loss = 0.0\n",
        "            Y = []\n",
        "            Y_pred = []\n",
        "            for batch_idx, (inputs, labels) in enumerate(validloader):\n",
        "                #print(\"Val batch_idx\",batch_idx)\n",
        "                inputs = inputs.view(inputs.size(0), -1).float()\n",
        "                if use_cuda:\n",
        "                    inputs = inputs.cuda()\n",
        "                inputs = Variable(inputs)\n",
        "                z, outputs, mu, logvar = self.forward(inputs)\n",
        "\n",
        "                loss = self.loss_function(outputs, inputs, z, mu, logvar)\n",
        "                valid_loss += loss.data.item()*len(inputs)\n",
        "                # total_loss += valid_recon_loss.data[0] * inputs.size()[0]\n",
        "                # total_num += inputs.size()[0]\n",
        "                gamma = self.get_gamma(z, mu, logvar).data.cpu().numpy()\n",
        "                Y.append(labels.cpu().numpy())\n",
        "                Y_pred.append(np.argmax(gamma, axis=1))\n",
        "\n",
        "                # view reconstruct\n",
        "                if visualize and batch_idx == 0:\n",
        "                    n = min(inputs.size(0), 8)\n",
        "                    comparison = torch.cat([inputs.view(-1, 1, 11, 20)[:n],\n",
        "                                            outputs.view(-1, 1, 11, 20)[:n]])\n",
        "                    save_image(comparison.data.cpu(),\n",
        "                                 'results/vae/reconstruct/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "            Y = np.concatenate(Y)\n",
        "            Y_pred = np.concatenate(Y_pred)\n",
        "            #print(\"getting cluster acc\")\n",
        "            acc = cluster_acc(Y_pred, Y)\n",
        "            # valid_loss = total_loss / total_num\n",
        "            print(\"#Epoch %3d: lr: %.5f, Train Loss: %.5f, Valid Loss: %.5f, acc: %.5f\" % (\n",
        "                epoch, epoch_lr, train_loss / len(trainloader.dataset), valid_loss / len(validloader.dataset), acc[0]))\n",
        "\n",
        "            # view sample\n",
        "            if visualize:\n",
        "                sample = Variable(torch.randn(LATENT_N, self.z_dim))\n",
        "                if use_cuda:\n",
        "                   sample = sample.cuda()\n",
        "                sample = self.decode(sample).cpu()\n",
        "                save_image(sample.data.view(LATENT_N, 11*20),\n",
        "                           'results/vae/sample/sample_' + str(epoch) + '.png')\n",
        "\n",
        "    def log_marginal_likelihood_estimate(self, x, num_samples):\n",
        "        weight = torch.zeros(x.size(0))\n",
        "        for i in range(num_samples):\n",
        "            z, recon_x, mu, logvar = self.forward(x)\n",
        "            zloglikelihood = log_likelihood_samples_unit_gaussian(z)\n",
        "            dataloglikelihood = torch.sum(x*torch.log(torch.clamp(recon_x, min=1e-10))+\n",
        "                (1-x)*torch.log(torch.clamp(1-recon_x, min=1e-10)), 1)\n",
        "            log_qz = log_likelihood_samplesImean_sigma(z, mu, logvar)\n",
        "            weight += torch.exp(dataloglikelihood + zloglikelihood - log_qz).data\n",
        "        # pdb.set_trace()\n",
        "        return torch.log(torch.clamp(weight/num_samples, min=1e-40))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwJUdPZwgqAT"
      },
      "source": [
        "class CDRH3MotifDataset(Dataset):\n",
        "    \"\"\"\n",
        "    pyTorch Dataloader conform class,\n",
        "    requires implementation of __len__ and __getitem__\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv_path: str, device: str = 'cuda:0'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_path (string): Path to the csv file with CDRH3 motifs.\n",
        "            device (string): device where data is kept in memory\n",
        "        \"\"\"\n",
        "        with open(csv_path, 'r') as file:\n",
        "            reader = csv.reader(file, delimiter='\\t')\n",
        "            next(reader)\n",
        "            classes = set()\n",
        "            data = list()\n",
        "            for line in reader:\n",
        "                data.append((line[0], line[1:]))\n",
        "                classes |= set(line[0])\n",
        "\n",
        "        encoding = {letter: np.eye(len(classes), dtype='float32')[i]\n",
        "                    for i, letter in enumerate(classes)}\n",
        "        label_encoding = {letter: i for i, letter in enumerate(classes)}\n",
        "        dataset = np.zeros((len(data), len(data[0][0]), len(classes)),\n",
        "                           dtype='int')\n",
        "        labels = np.zeros((len(data), len(data[0][0])), dtype='int64')\n",
        "        eval_data = dict()\n",
        "        for i, line in enumerate(data):\n",
        "            for jdx, letter in enumerate(line[0]):\n",
        "                dataset[i, jdx, :] = encoding[letter]\n",
        "                labels[i, jdx] = label_encoding[letter]\n",
        "                eval_data[i] = line[1]\n",
        "\n",
        "        # transform numpy array to torch tensor\n",
        "        # and copy it on the correct device\n",
        "        self.dataset = torch.from_numpy(dataset).to(device)\n",
        "        self.labels = torch.from_numpy(labels).to(device)\n",
        "        self.eval_data = eval_data\n",
        "        self.encoding = encoding\n",
        "        self.label_encoding = label_encoding\n",
        "        self.reverse_encoding = {repr(i): j for j, i in encoding.items()}\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataset)\n",
        "\n",
        "    \"\"\"\n",
        "    __getitem__\n",
        "    Output:\n",
        "        (OneHotEncoding of data, IntegerEncoding of data,\n",
        "        list with epitope, structure and antigen)\n",
        "    \"\"\"\n",
        "    def __getitem__(self, idx: int) -> (FloatTensor):\n",
        "        return (self.dataset[idx, :, :])\n",
        "\n",
        "class CDRH3MotifDataset_labels(Dataset):\n",
        "    \"\"\"\n",
        "    pyTorch Dataloader conform class,\n",
        "    requires implementation of __len__ and __getitem__\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv_path: str, device: str = 'cuda:0'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_path (string): Path to the csv file with CDRH3 motifs.\n",
        "            device (string): device where data is kept in memory\n",
        "        \"\"\"\n",
        "        with open(csv_path, 'r') as file:\n",
        "            reader = csv.reader(file, delimiter='\\t')\n",
        "            next(reader)\n",
        "            classes = set()\n",
        "            data = list()\n",
        "            for line in reader:\n",
        "                data.append((line[0], line[1:]))\n",
        "                classes |= set(line[0])\n",
        "\n",
        "        encoding = {letter: np.eye(len(classes), dtype='float32')[i]\n",
        "                    for i, letter in enumerate(classes)}\n",
        "        label_encoding = {letter: i for i, letter in enumerate(classes)}\n",
        "        dataset = np.zeros((len(data), len(data[0][0]), len(classes)),\n",
        "                           dtype='int')\n",
        "        labels = np.zeros((len(data), len(data[0][0])), dtype='int64')\n",
        "        eval_data = dict()\n",
        "        for i, line in enumerate(data):\n",
        "            for jdx, letter in enumerate(line[0]):\n",
        "                dataset[i, jdx, :] = encoding[letter]\n",
        "                labels[i, jdx] = label_encoding[letter]\n",
        "                eval_data[i] = line[1]\n",
        "\n",
        "        # transform numpy array to torch tensor\n",
        "        # and copy it on the correct device\n",
        "        self.dataset = torch.from_numpy(dataset).to(device)\n",
        "        self.labels = torch.from_numpy(labels).to(device)\n",
        "        self.eval_data = eval_data\n",
        "        self.encoding = encoding\n",
        "        self.label_encoding = label_encoding\n",
        "        self.reverse_encoding = {repr(i): j for j, i in encoding.items()}\n",
        "        with open(DATA_PATH, 'r') as file:\n",
        "            reader = csv.reader(file, delimiter='\\t')\n",
        "            next(reader)\n",
        "            classes = set()\n",
        "            data = list()\n",
        "            for line in reader:\n",
        "                data.append((line[0], line[1:]))\n",
        "                classes |= set(line[0])\n",
        "        labels = [x[1][2] for x in data]\n",
        "        ag = labels\n",
        "        ag_set = set()\n",
        "        for element in ag:\n",
        "            ag_set |= {element}\n",
        "        ag_map = {name: i for i, name in enumerate(ag_set)}\n",
        "        self.ag_color = torch.Tensor([ag_map[s] for s in ag]).to(device)\n",
        "        print(self.ag_color.shape)\n",
        "        self.dataset = self.dataset[1:10000,:,:]\n",
        "        self.ag_color = self.ag_color[1:10000]\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataset)\n",
        "\n",
        "    \"\"\"\n",
        "    __getitem__\n",
        "    Output:\n",
        "        (OneHotEncoding of data, IntegerEncoding of data,\n",
        "        list with epitope, structure and antigen)\n",
        "    \"\"\"\n",
        "    def __getitem__(self, idx: int) -> (FloatTensor):\n",
        "        return (self.dataset[idx, :, :],self.ag_color[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_vCqFebiKgz"
      },
      "source": [
        "dataset = CDRH3MotifDataset(DATA_PATH, device=DEVICE)\n",
        "dataset_val = CDRH3MotifDataset_labels(DATA_PATH, device = DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJvrM7tViejq"
      },
      "source": [
        "train_data = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0,\n",
        "                    drop_last = False)\n",
        "val_data = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n",
        "                    drop_last = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qd_qTcDubg3"
      },
      "source": [
        "!mkdir -p results/vae/reconstruct\n",
        "!mkdir -p results/vae/sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj1rc5EVSg6m"
      },
      "source": [
        "vade = VaDE(input_dim=11*20, z_dim=LATENT_N, n_centroids=10, binary=True,\n",
        "        encodeLayer=[200,200,400], decodeLayer=[400,200,200])\n",
        "\n",
        "print(\"Initializing through GMM..\")\n",
        "vade.initialize_gmm(val_data)\n",
        "vade.fit(train_data, val_data, lr=0.001, batch_size=BATCH_SIZE, num_epochs=EPOCHS, anneal=True,visualize = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqSEF1OV0H-I"
      },
      "source": [
        "!ls results/vae/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE_yAkxBygIB"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('results/vae/reconstruct/reconstruction_0.png',width=400,height=800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7tV7jxw0ZzQ"
      },
      "source": [
        "Image('results/vae/reconstruct/reconstruction_1.png',width=400,height=800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGjORZ9C0c1G"
      },
      "source": [
        "Image('results/vae/reconstruct/reconstruction_2.png',width=400,height=800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYMRMovay4Jk"
      },
      "source": [
        "Image('results/vae/reconstruct/reconstruction_3.png',width=400,height=800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vuNQppq0ivT"
      },
      "source": [
        "Image('results/vae/reconstruct/reconstruction_4.png',width=400,height=800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWEApLK0kxL"
      },
      "source": [
        "Image('results/vae/sample/sample_4.png',width=400,height=800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97bnqe4CwAXz"
      },
      "source": [
        "all_data = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n",
        "                    drop_last = False)\n",
        "all_z = np.zeros((len(dataset),LATENT_N),np.float32)\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs) in enumerate(all_data):\n",
        "        inputs = inputs.view(inputs.size(0), -1).float()\n",
        "        z = vade.forward(inputs)\n",
        "        if z[0].shape[0] == BATCH_SIZE:\n",
        "            all_z[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE,:] = z[0].cpu().numpy()\n",
        "        else:\n",
        "            all_z[batch_idx*BATCH_SIZE:len(dataset),:] = z[0].cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFs2oAk0xFCT"
      },
      "source": [
        "do_eval(all_z,do_umap=False)    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}